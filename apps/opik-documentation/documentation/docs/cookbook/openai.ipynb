{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_hA6AYXHGh"
      },
      "source": [
        "# Using Opik with OpenAI\n",
        "\n",
        "Opik integrates with OpenAI to provide a simple way to log traces for all OpenAI LLM calls. This works for all OpenAI models, including if you are using the streaming API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ruy-f7BXHGs"
      },
      "source": [
        "## Creating an account on Comet.com\n",
        "\n",
        "[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) and grab your API Key.\n",
        "\n",
        "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcxGWVzXHGw"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade opik openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqLNpmO3XHG1",
        "outputId": "0eab5998-24ce-4326-8871-5f1d73a2c325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Your Opik API key is available in your account settings, can be found at https://www.comet.com/api/my/settings/ for Opik cloud\n"
          ]
        }
      ],
      "source": [
        "import opik\n",
        "\n",
        "opik.configure(use_local=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M6S7acJXHG3"
      },
      "source": [
        "## Preparing our environment\n",
        "\n",
        "First, we will set up our OpenAI API keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4jIm2DrXHG5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB8R351oXHG6"
      },
      "source": [
        "## Logging traces\n",
        "\n",
        "In order to log traces to Opik, we need to wrap our OpenAI calls with the `track_openai` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGmy6JAMXHG8"
      },
      "outputs": [],
      "source": [
        "from opik.integrations.openai import track_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"openai-integration-demo\"\n",
        "\n",
        "client = OpenAI()\n",
        "openai_client = track_openai(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJd0vl0YXHG_"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Write a short two sentence story about Opik.\n",
        "\"\"\"\n",
        "\n",
        "completion = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UYzMZAXXHHC"
      },
      "source": [
        "The prompt and response messages are automatically logged to Opik and can be viewed in the UI.\n",
        "\n",
        "![OpenAI Integration](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/openai_trace_cookbook.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65KzuiBXXHHF"
      },
      "source": [
        "## Using it with the `track` decorator\n",
        "\n",
        "If you have multiple steps in your LLM pipeline, you can use the `track` decorator to log the traces for each step. If OpenAI is called within one of these steps, the LLM call with be associated with that corresponding step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRednsFyXHHG"
      },
      "outputs": [],
      "source": [
        "from opik import track\n",
        "from opik.integrations.openai import track_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"openai-integration-demo\"\n",
        "\n",
        "client = OpenAI()\n",
        "openai_client = track_openai(client)\n",
        "\n",
        "\n",
        "@track\n",
        "def generate_story(prompt):\n",
        "    res = openai_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return res.choices[0].message.content\n",
        "\n",
        "\n",
        "@track\n",
        "def generate_topic():\n",
        "    prompt = \"Generate a topic for a story about Opik.\"\n",
        "    res = openai_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return res.choices[0].message.content\n",
        "\n",
        "\n",
        "@track\n",
        "def generate_opik_story():\n",
        "    topic = generate_topic()\n",
        "    story = generate_story(topic)\n",
        "    return story\n",
        "\n",
        "\n",
        "generate_opik_story()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbqCiKMXHHI"
      },
      "source": [
        "The trace can now be viewed in the UI:\n",
        "\n",
        "![OpenAI Integration](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/openai_trace_decorator_cookbook.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIJdj_vxXHHJ"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}